---
kep-number: 29
title: Node Fencing KEP
authors:
  - "@beekhof"
owning-sig: sig-node
participating-sigs:
  - sig-cluster-lifecycle
  - sig-storage
reviewers:
  - TBD
  - "@thockin"
  - "@smarterclayton"
approvers:
  - TBD
editor: TBD
creation-date: 2018-09-19
last-updated: 2018-09-19
status: provisional
see-also:
replaces:
superseded-by:
---

# Node Fencing

## Table of Contents

* [Table of Contents](#table-of-contents)
* [Summary](#summary)
* [Motivation](#motivation)
    * [Goals](#goals)
    * [Non-Goals](#non-goals)
* [Proposal](#proposal)
    * [Implementation Details/Notes/Constraints [optional]](#implementation-detailsnotesconstraints-optional)
    * [Risks and Mitigations](#risks-and-mitigations)
* [Graduation Criteria](#graduation-criteria)
* [Implementation History](#implementation-history)
* [Drawbacks [optional]](#drawbacks-optional)
* [Alternatives [optional]](#alternatives-optional)

## Summary

The loss of a worker node should be transparent to users of StatefulSets and
RWO Volumes. Recovery time for affected Pods and Volumes should be bounded and
short, allowing workloads to be recovered elsewhere and scale up/down events
to proceed.

## Motivation

Neither software, the hardware it runs on, nor the networks that connect them
are perfect.  

Attempts to automatically preserve availabilty in the presence of errors is
hampered by failures of peer hardware being generally indistinguishable from
the failure of network and/or software stack.

Using other communication paths can allow the system to know that the peer is
alive in some capacity, but unless the peer can take steps to recover itself
in a timely manner, this information is of little use.

Unlike ReplicaSets which can assume the best and spin up new instances, this
has problematic consequences for anything seeking to provide at-most-one
semantics.  Especially Stateful Sets which cannot scale up or down while one
of its members is in a failed state.

Fencing is a well established concept that allows a system to know for sure
that it is safe to recover these kinds of workloads to other peers.
Fundamentally it answers the question: _Is this peer capabale of corrupting my
data?_ With a resounding _No_, and based on that the system can then safely
initiate recovery.

The traditional non-cloud mechansim for this is to power off the physical
hardware.  More optimistic implementions power cycle the hardware in the hope
that it will return to a functional state.  Other more complex systems will
seek to isolate the peer from the network (so that no additional requests can
be consumed) and the disk (preventing the double writers problem) in order to
allow triage activities to take place.

### Experience reports

* Cheng Xing [AttachDetachController fails to force detach on pod deletion](https://github.com/kubernetes/kubernetes/issues/65392) June 2018
* Clayton Colman [Pod Safety](https://github.com/kubernetes/community/blob/16f88595883a7461010b6708fb0e0bf1b046cf33/contributors/design-proposals/pod-safety.md) October 2016


### Goals

* Specify where implementations should look in order to determine if a node requires fencing
* Specify how implementations should indicate a node needs to be fenced
* Specify how implementations should indicate a node has been sucessfully fenced
* Specify how nodes should return to a "normal" state
* Specify the changes required to Kubernetes that enable automated recovery in the presense of worker failures
* Provide a sample implementation based on the [Machine](https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/proposals/machine-api-proposal.md) API 

### Non-Goals

It is explicitly _not_ the goal of this work to:

* Have fencing become a requirement for all Kubernetes deployments
* Have the community converge on a particular fencing implementation

## Proposal

Three additional NodeConditions and taints:

* a `node.alpha.kubernetes.io/fencingtriaged` taint corresponding to NodeCondition `FencingTriaged` being
   `True`
* a `node.alpha.kubernetes.io/fencingrequired` taint corresponding to NodeCondition `FencingRequired` being
   `True`
* a `node.alpha.kubernetes.io/fencingcomplete` taint corresponding to NodeCondition `FencingComplete` being
   `True`

This allows administrators and other system components to differentiate between:

* Nodes expected to be operational () 
* Nodes in an error state that have been triaged but do not require fencing (yet) (`FencingTriaged` = `True`, `FencingRequired` = `False` )
* Nodes in an error state that have been triaged (`FencingTriaged` = `True`, `FencingRequired` = `True` )
* Nodes in an error state that are being recovered (`FencingTriaged` = `True`, `FencingRequired` = `True`, `FencingComplete` = `False` )
* Nodes with errors that have entered a safe state (`FencingTriaged` = `True`, `FencingRequired` = `True`, `FencingComplete` = `True` )

Setting and unsetting the NodeConditions form the primary contract between
implementations and the rest of the system.

1. Implementations should use NodeConditions in order to determine if a node requires fencing.

   If additional information is required, it should be made available by
   augmenting the existing the list of NodeConditions using something like
   [Node Problem Detector](https://github.com/kubernetes/node-problem-detector/)

   Including error condition detection in fencing implementations should be
   avoided.

1. Implementations should indicate a Node needs to be fenced with the `NeedsFencing` NodeCondition.

   Admins may also set NodeCondition `FencingRequired` to `True` to manually
   trigger fencing.

   Once fencing has been initiated, the NodeCondition `FencingComplete` should
   be set to `False`.

1. Implementations should indicate a Node has been sucessfully fenced by
   setting the NodeCondition `FencingComplete` to `True`.

   Fencing implementations must only set `FencingComplete`  once it confirms
   any workloads on the node have been isolated from the network and any
   shared resources.  Most commonly this will mean the node has been powered
   off.

1. Implementations should return Nodes to a "normal" state by removing the
   `FencingComplete` and `FencingRequired` NodeConditions.

   This could occur periodically or more likely, when the NodeConditions that
   were determined to require fencing are no longer present.

1. Kubernetes Controllers should look for `FencingRequired` = `True` **and**
   `FencingComplete` = `True` to determine that it is safe to forcefully recover
   resources.

### Implementation Details/Notes/Constraints

#### Node Conditions

As setting and unsetting the `NodeConditions` form the primary contract between
implementations and the rest of the system, implementations should consider
failure modes that include or co-incide with interruptions to the service
network between nodes and `apiservers`.

For additional inputs to the repair logic, we rely on NPD which runs locally
on each node. In the presence of a service network failure, NPD would not be
able to set additional `NodeConditions`. However the fact that the node is
disconnected should already be sufficient to trigger fencing of the affected
nodes.

Since the fencing implementation is responsible for setting and clearing
`FencingTriaged`, `FencingRequired`, `FencingComplete`,  any disruption to the
service network between it and the `apiservers` is also important to consider.

In our implementation there are multiple copies that vote for a leader, the
presence of a service network failure affecting the elected controller results
in it being unable to renew its lease and one of its peers (on another node
that by definitiion still has connectivity) taking over and being able to set
`NodeConditions`.

In the presence of a total failure, then we shouldn't be fencing anyway so
there would be no need to set any `NodeConditions`.

#### Power Management

Since the most common form of fencing involves powering off nodes, co-
ordination with other entities that may be involved with power management
should be considered...

TBC

### Risks and Mitigations

TBA

## Graduation Criteria

TBA

## Implementation History

- Initial version: 2018-09-19

## Drawbacks

* In deleting the Node object, we are also removing the history of events that
triggered fencing.  Although this would be recorded in logs and optionally by
the fencing logic.

## Alternatives

* The fencing logic could delete Nodes once fencing is complete.  This might
be sufficient for existing controllers such as AttachDetachController to
trigger cleanup logic


## Infrastructure Needed [optional]

* A repository for the logic that triggers fencing
* A branch in which changes to Kubernetes itself can be evaluated
